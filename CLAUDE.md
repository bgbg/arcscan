# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

ArcScan is a YouTube video sentiment analysis platform that combines a Next.js frontend with a FastAPI Python backend. The application downloads YouTube videos, transcribes audio using OpenAI's Whisper, performs sentiment and emotion analysis using transformer models, and displays interactive visualizations with Firebase integration for data persistence.

## Architecture

### Monorepo Structure

- **`/arcscan`**: Next.js 15 frontend application (TypeScript/React)
- **`/backend`**: FastAPI Python backend (`app.py`)
- **Root level**: Utility Python scripts for testing (`main.py`, `test_whisper.py`, `firebase_test.py`)

### Frontend (Next.js App)

The frontend uses the App Router pattern:

- **Authentication Flow**: Firebase Auth with email/password
  - [`/app/login`](arcscan/app/login), [`/app/register`](arcscan/app/register), [`/app/reset-password`](arcscan/app/reset-password), [`/app/verify`](arcscan/app/verify)
  - [ProtectedRoute.tsx](arcscan/components/ProtectedRoute.tsx) wraps authenticated pages
  - Firebase config in [lib/firebaseConfig.ts](arcscan/lib/firebaseConfig.ts)

- **Main Dashboard**: [`/app/dashboard/page.tsx`](arcscan/app/dashboard/page.tsx)
  - Users input YouTube URL
  - Frontend polls backend via Next.js API routes for analysis progress
  - Real-time progress updates via Firebase Firestore listeners
  - Displays sentiment timeline, emotion breakdowns, and transcripts

- **API Routes** (proxy to Python backend):
  - [`/app/api/analyze/advanced-emotions/route.js`](arcscan/app/api/analyze/advanced-emotions/route.js)
  - [`/app/api/progress/advanced/[videoUrl]/route.js`](arcscan/app/api/progress/advanced/[videoUrl]/route.js)
  - [`/app/api/results/advanced/[videoUrl]/route.js`](arcscan/app/api/results/advanced/[videoUrl]/route.js)
  - All routes proxy to FastAPI backend using `process.env.BACKEND_URL`

- **Key Components**:
  - [YouTubePlayer.jsx](arcscan/components/YouTubePlayer.jsx) - Embedded video player with time syncing
  - [SentimentTimeline.jsx](arcscan/components/SentimentTimeline.jsx) - Charts for sentiment over time
  - [AdvancedEmotions.jsx](arcscan/components/AdvancedEmotions.jsx) - Emotion breakdown visualization
  - [AnalysisProgress.jsx](arcscan/components/AnalysisProgress.jsx) - Real-time progress tracker
  - [TranslatedTranscript.jsx](arcscan/components/TranslatedTranscript.jsx) - Shows original/translated text
  - [PDFExportButton.jsx](arcscan/components/PDFExportButton.jsx) - Export analysis to PDF

- **History & Profile**:
  - [`/app/dashboard/history/page.tsx`](arcscan/app/dashboard/history/page.tsx) - View past analyses
  - [`/app/dashboard/history/[id]/page.tsx`](arcscan/app/dashboard/history/[id]/page.tsx) - Individual analysis detail
  - [`/app/dashboard/profile/page.tsx`](arcscan/app/dashboard/profile/page.tsx) - User profile
  - [`/app/dashboard/settings/page.tsx`](arcscan/app/dashboard/settings/page.tsx) - User settings

### Backend (FastAPI)

Located in [`/backend/app.py`](backend/app.py). Key functionality:

- **Video Processing Pipeline**:
  1. Download YouTube audio using `yt_dlp`
  2. Transcribe with OpenAI Whisper API (with timestamps)
  3. Language detection using `langdetect`
  4. Auto-translate Hebrew/Arabic to English using GPT-3.5-turbo
  5. Sentiment analysis using `cardiffnlp/twitter-roberta-base-sentiment`
  6. Advanced emotion analysis using GoEmotions model (28 emotions based on Plutchik's wheel)
  7. Timeline data generation with smoothing algorithms
  8. Store results in Firebase Firestore

- **Firebase Integration**:
  - Uses `firebase_admin` SDK with service account credentials (`firebase_credentials.json`)
  - Collections:
    - `analyses` - Completed analysis results (indexed by video URL)
    - `analysis_progress` - Real-time progress updates for active analyses
  - Document IDs generated by cleaning video URLs (removes special chars, time markers)

- **Key Endpoints**:
  - `POST /analyze` - Main analysis endpoint
  - `POST /analyze/advanced-emotions` - Advanced emotion analysis
  - `GET /progress/{video_url:path}` - Get analysis progress
  - `GET /results/{video_url:path}` - Retrieve completed analysis
  - `GET /history/{user_id}` - Get user's analysis history

- **Progress Tracking**: Backend updates Firestore `analysis_progress` collection at each pipeline stage (downloading, transcribing, analyzing, creating_timeline, summarizing, complete)

- **Caching Strategy**: Analyses are cached by video URL - if a video was already analyzed, results are returned immediately without reprocessing

### SQLite Batch Database (Schema V2)

Located in [`/backend/batch_results.db`](backend/batch_results.db). The batch processing system uses a normalized SQLite schema for efficient querying and time-series analysis.

- **Database Schema** ([schema_v2.sql](backend/schema_v2.sql)):

  **Core Tables:**
  - `video_metadata` - Video information and processing status
    - Columns: `url` (PK), `date`, `person_name`, `title`, `created_at`, `updated_at`, `status`, `error_message`, `analysis_json` (backward compatibility)
    - Indexes: `person_name`, `date`, `status`, `(person_name, date)`

  - `transcriptions` - Transcription data with language detection
    - Columns: `id` (PK), `video_url` (FK), `text`, `language`, `source` (whisper/subtitles), `detected_language`, `subtitle_language`, `decision_path`, `created_at`
    - Supports multiple transcriptions per video
    - Indexes: `video_url`, `language`, `source`

  - `translations` - Translation metadata for non-English content
    - Columns: `id` (PK), `transcription_id` (FK), `original_text`, `translated_text`, `model`, `created_at`

  - `sentences` - Sentence-level data with timestamps and sentiment
    - Columns: `id` (PK), `video_url` (FK), `sentence_index`, `text`, `start_time`, `end_time`, `sentiment`, `created_at`
    - Enables timeline aggregation at any granularity
    - Indexes: `video_url`, `sentiment`, `start_time`, `end_time`, `(video_url, start_time, end_time)`

  - `video_sentiments` - Aggregated sentiment statistics per video
    - Columns: `video_url` (PK FK), `overall_sentiment`, `positive_count`, `positive_pct`, `neutral_count`, `neutral_pct`, `negative_count`, `negative_pct`, `created_at`

  - `sentence_emotions` - Emotion analysis at sentence level (GoEmotions model)
    - Columns: `id` (PK), `sentence_id` (FK), `emotion_name`, `score`, `created_at`
    - Supports 28 emotions based on Plutchik's wheel

  - `video_emotion_summary` - Aggregated emotion statistics per video
    - 28 emotion columns (joy, sadness, anger, fear, surprise, disgust, etc.) with average scores

- **Batch Processing** ([batch_processor.py](backend/batch_processor.py)):
  - `process_single_video()` - Process individual videos
  - `process_video_batch()` - Batch process multiple videos from CSV
  - `sync_to_firebase()` - Sync SQLite results to Firebase Firestore
  - All analysis results are persisted to database (output files are exports only)

- **Database Operations** ([batch_db.py](backend/batch_db.py)):
  - **Write functions:**
    - `save_video_analysis()` - Save complete analysis to normalized tables
    - `save_transcription()`, `save_translation()`, `save_sentences()`, `save_sentiments()`, `save_emotions()` - Helper functions
  - **Query functions:**
    - `get_all_results()` - All videos with metadata
    - `get_videos_by_person()` - Filter by person name
    - `get_date_range_results()` - Filter by date range
    - `get_sentiment_statistics()` - Overall sentiment distribution
    - `get_video_timeline()` - Sentiment timeline with configurable bucket size
    - `get_person_sentiment_trends()` - Time-series sentiment data
    - `get_emotion_breakdown()` - Sentence and video-level emotion analysis
  - All functions support both V2 (normalized) and V1 (legacy JSON) schemas with automatic detection

- **Batch API Endpoints**:
  - `POST /batch/analyze` - Analyze videos from CSV data
  - `POST /batch/upload` - Upload and parse CSV file
  - `GET /batch/results` - Query analysis results (supports filters: person, date range, status)
  - `GET /batch/people` - List all people with statistics
  - `GET /batch/statistics` - Overall sentiment statistics
  - `GET /batch/video/{video_url:path}/timeline` - Sentiment timeline for specific video
  - `GET /batch/person/{name}/trends` - Sentiment trends over time for person
  - `GET /batch/video/{video_url:path}/emotions` - Emotion analysis breakdown

- **Schema Migration** ([migrate_schema.py](backend/migrate_schema.py)):
  - Migrates from V1 (monolithic JSON) to V2 (normalized tables)
  - Features:
    - Automatic backup creation (`videos_backup` table)
    - Dry-run mode for validation
    - Rollback capability
    - Detailed migration report with statistics
    - Transaction safety and error handling
  - Usage:
    ```bash
    # Dry run (validation only)
    python backend/migrate_schema.py --dry-run

    # Run migration
    python backend/migrate_schema.py --db-path backend/batch_results.db

    # Rollback if needed
    python backend/migrate_schema.py --rollback
    ```

- **Testing** ([test_schema_migration.py](backend/test_schema_migration.py)):
  ```bash
  pytest backend/test_schema_migration.py -v
  ```

## Development Commands

### Frontend (Next.js)

```bash
cd arcscan
npm install              # Install dependencies
npm run dev             # Start dev server on http://localhost:3000
npm run build           # Build for production
npm run start           # Start production server
npm run lint            # Run ESLint
```

### Backend (FastAPI)

```bash
# Install dependencies:
cd backend
pip install -r requirements.txt

# Run backend server:
uvicorn app:app --reload --port 8000
# Backend runs on http://localhost:8000
```

### Environment Configuration

Create `.env.local` in `/arcscan`:
```
NEXT_PUBLIC_FIREBASE_API_KEY=...
NEXT_PUBLIC_FIREBASE_AUTH_DOMAIN=...
NEXT_PUBLIC_FIREBASE_PROJECT_ID=...
NEXT_PUBLIC_FIREBASE_STORAGE_BUCKET=...
NEXT_PUBLIC_FIREBASE_MESSAGING_SENDER_ID=...
NEXT_PUBLIC_FIREBASE_APP_ID=...
BACKEND_URL=http://localhost:8000
```

Create `.env` in `/backend` (see `.env.example` for template):
```
OPENAI_API_KEY=...
LANGSMITH_API_KEY=...
LANGSMITH_PROJECT=arcscan
LANGSMITH_TRACING=true
```

**LangSmith Configuration**:
- `LANGSMITH_API_KEY` - **Required**. API key for LangSmith tracing (get from [LangSmith dashboard](https://smith.langchain.com)). Application will fail to start if missing. This requirement is intentional to ensure all LLM API calls are traced for monitoring and cost tracking.
- `LANGSMITH_PROJECT` - Project name for organizing traces (default: `arcscan`)
- `LANGSMITH_TRACING` - Enable/disable tracing (`true`/`false`, default: `false`)

LangSmith automatically traces all OpenAI API calls (Whisper transcriptions and GPT-3.5-turbo translations), capturing:
- Model names and parameters
- Token usage and costs
- Request/response latency
- Input/output content
- Error details

View traces at [https://smith.langchain.com](https://smith.langchain.com)

Backend also requires `firebase_credentials.json` in `/backend`.

## Key Technical Details

### URL Cleaning & Document ID Generation
- Video URLs are cleaned to remove time markers (`?t=123`)
- Special characters are replaced with underscores to create Firestore-safe document IDs
- This ensures the same video always maps to the same document regardless of URL parameters

### Translation Flow
- Whisper transcribes in original language
- `langdetect` identifies language
- Only Hebrew (`he`) and Arabic (`ar`) trigger GPT-3.5-turbo translation
- Both original and translated text are stored in Firestore
- Sentiment analysis runs on translated English text

### Real-time Updates
- Frontend sets up Firestore `onSnapshot` listener on `analysis_progress/{docId}`
- Backend updates progress document after each pipeline stage
- Frontend receives real-time updates without polling HTTP endpoints

### Sentiment Models
- **Basic sentiment**: `cardiffnlp/twitter-roberta-base-sentiment` (Positive/Neutral/Negative)
- **Advanced emotions**: GoEmotions model tracking 28 emotions (joy, sadness, anger, fear, surprise, etc.)
- Timeline smoothing applied to reduce noise in sentiment visualizations

### UI Framework
- Tailwind CSS with custom config ([tailwind.config.ts](arcscan/tailwind.config.ts))
- Radix UI components for accessible primitives
- shadcn/ui component library (configured in [components.json](arcscan/components.json))
- Dark mode support via `class` strategy

## File Naming Conventions

- Frontend uses mixed `.tsx` and `.jsx` extensions
- Components that don't use TypeScript features are `.jsx` (e.g., visualization components)
- Page routes and TypeScript-heavy components are `.tsx`
- Backend API routes in Next.js are `.js` files

## Common Patterns

### Protected Routes
Wrap any authenticated page with:
```tsx
import ProtectedRoute from '@/components/ProtectedRoute'

export default function Page() {
  return (
    <ProtectedRoute>
      {/* Your page content */}
    </ProtectedRoute>
  )
}
```

### Firebase Data Access
```tsx
import { db, auth } from '@/lib/firebaseConfig'
import { useAuthState } from 'react-firebase-hooks/auth'
import { collection, doc, getDoc, getDocs } from 'firebase/firestore'
```

### Backend URL Configuration
Next.js API routes use `process.env.BACKEND_URL || 'http://localhost:8000'` to proxy requests to FastAPI.

## Testing & Analysis Scripts

- [`/main.py`](main.py) - Standalone YouTube audio downloader utility
- [`/test_whisper.py`](test_whisper.py) - Test OpenAI Whisper transcription
- [`/firebase_test.py`](firebase_test.py) - Test Firebase connectivity

## Important Notes

- The project is currently in testing phase (per root README)
- Backend expects `firebase_credentials.json` to be present (not in git)
- OpenAI API key required for Whisper and translation features
- FFmpeg must be installed on system for `yt_dlp` audio extraction
- Next.js uses `devIndicators: false` in config to hide dev overlay
