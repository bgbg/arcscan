# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

ArcScan is a YouTube video sentiment analysis platform that combines a Next.js frontend with a FastAPI Python backend. The application downloads YouTube videos, transcribes audio using OpenAI's Whisper, performs sentiment and emotion analysis using transformer models, and displays interactive visualizations with Firebase integration for data persistence.

## Architecture

### Monorepo Structure

- **`/arcscan`**: Next.js 15 frontend application (TypeScript/React)
- **`/backend`**: FastAPI Python backend (`app.py`)
- **Root level**: Utility Python scripts for testing (`main.py`, `test_whisper.py`, `firebase_test.py`)

### Frontend (Next.js App)

The frontend uses the App Router pattern:

- **Authentication Flow**: Firebase Auth with email/password
  - [`/app/login`](arcscan/app/login), [`/app/register`](arcscan/app/register), [`/app/reset-password`](arcscan/app/reset-password), [`/app/verify`](arcscan/app/verify)
  - [ProtectedRoute.tsx](arcscan/components/ProtectedRoute.tsx) wraps authenticated pages
  - Firebase config in [lib/firebaseConfig.ts](arcscan/lib/firebaseConfig.ts)

- **Main Dashboard**: [`/app/dashboard/page.tsx`](arcscan/app/dashboard/page.tsx)
  - Users input YouTube URL
  - Frontend polls backend via Next.js API routes for analysis progress
  - Real-time progress updates via Firebase Firestore listeners
  - Displays sentiment timeline, emotion breakdowns, and transcripts

- **API Routes** (proxy to Python backend):
  - [`/app/api/analyze/advanced-emotions/route.js`](arcscan/app/api/analyze/advanced-emotions/route.js)
  - [`/app/api/progress/advanced/[videoUrl]/route.js`](arcscan/app/api/progress/advanced/[videoUrl]/route.js)
  - [`/app/api/results/advanced/[videoUrl]/route.js`](arcscan/app/api/results/advanced/[videoUrl]/route.js)
  - All routes proxy to FastAPI backend using `process.env.BACKEND_URL`

- **Key Components**:
  - [YouTubePlayer.jsx](arcscan/components/YouTubePlayer.jsx) - Embedded video player with time syncing
  - [SentimentTimeline.jsx](arcscan/components/SentimentTimeline.jsx) - Charts for sentiment over time
  - [AdvancedEmotions.jsx](arcscan/components/AdvancedEmotions.jsx) - Emotion breakdown visualization
  - [AnalysisProgress.jsx](arcscan/components/AnalysisProgress.jsx) - Real-time progress tracker
  - [TranslatedTranscript.jsx](arcscan/components/TranslatedTranscript.jsx) - Shows original/translated text
  - [PDFExportButton.jsx](arcscan/components/PDFExportButton.jsx) - Export analysis to PDF

- **History & Profile**:
  - [`/app/dashboard/history/page.tsx`](arcscan/app/dashboard/history/page.tsx) - View past analyses
  - [`/app/dashboard/history/[id]/page.tsx`](arcscan/app/dashboard/history/[id]/page.tsx) - Individual analysis detail
  - [`/app/dashboard/profile/page.tsx`](arcscan/app/dashboard/profile/page.tsx) - User profile
  - [`/app/dashboard/settings/page.tsx`](arcscan/app/dashboard/settings/page.tsx) - User settings

### Backend (FastAPI)

Located in [`/backend/app.py`](backend/app.py). Key functionality:

- **Video Processing Pipeline**:
  1. Download YouTube audio using `yt_dlp`
  2. Transcribe with OpenAI Whisper API (with timestamps)
  3. Language detection using `langdetect`
  4. Auto-translate Hebrew/Arabic to English using GPT-3.5-turbo
  5. Sentiment analysis using `cardiffnlp/twitter-roberta-base-sentiment`
  6. Advanced emotion analysis using GoEmotions model (28 emotions based on Plutchik's wheel)
  7. Timeline data generation with smoothing algorithms
  8. Store results in Firebase Firestore

- **Firebase Integration**:
  - Uses `firebase_admin` SDK with service account credentials (`firebase_credentials.json`)
  - Collections:
    - `analyses` - Completed analysis results (indexed by video URL)
    - `analysis_progress` - Real-time progress updates for active analyses
  - Document IDs generated by cleaning video URLs (removes special chars, time markers)

- **Key Endpoints**:
  - `POST /analyze` - Main analysis endpoint
  - `POST /analyze/advanced-emotions` - Advanced emotion analysis
  - `GET /progress/{video_url:path}` - Get analysis progress
  - `GET /results/{video_url:path}` - Retrieve completed analysis
  - `GET /history/{user_id}` - Get user's analysis history

- **Progress Tracking**: Backend updates Firestore `analysis_progress` collection at each pipeline stage (downloading, transcribing, analyzing, creating_timeline, summarizing, complete)

- **Caching Strategy**: Analyses are cached by video URL - if a video was already analyzed, results are returned immediately without reprocessing

## Development Commands

### Frontend (Next.js)

```bash
cd arcscan
npm install              # Install dependencies
npm run dev             # Start dev server on http://localhost:3000
npm run build           # Build for production
npm run start           # Start production server
npm run lint            # Run ESLint
```

### Backend (FastAPI)

```bash
# Install dependencies (no requirements.txt found - manually install):
pip install fastapi uvicorn python-dotenv openai yt-dlp transformers firebase-admin langdetect

# Run backend server:
cd backend
uvicorn app:app --reload --port 8000
# Backend runs on http://localhost:8000
```

### Environment Configuration

Create `.env.local` in `/arcscan`:
```
NEXT_PUBLIC_FIREBASE_API_KEY=...
NEXT_PUBLIC_FIREBASE_AUTH_DOMAIN=...
NEXT_PUBLIC_FIREBASE_PROJECT_ID=...
NEXT_PUBLIC_FIREBASE_STORAGE_BUCKET=...
NEXT_PUBLIC_FIREBASE_MESSAGING_SENDER_ID=...
NEXT_PUBLIC_FIREBASE_APP_ID=...
BACKEND_URL=http://localhost:8000
```

Create `.env` in `/backend`:
```
OPENAI_API_KEY=...
```

Backend also requires `firebase_credentials.json` in `/backend`.

## Key Technical Details

### URL Cleaning & Document ID Generation
- Video URLs are cleaned to remove time markers (`?t=123`)
- Special characters are replaced with underscores to create Firestore-safe document IDs
- This ensures the same video always maps to the same document regardless of URL parameters

### Translation Flow
- Whisper transcribes in original language
- `langdetect` identifies language
- Only Hebrew (`he`) and Arabic (`ar`) trigger GPT-3.5-turbo translation
- Both original and translated text are stored in Firestore
- Sentiment analysis runs on translated English text

### Real-time Updates
- Frontend sets up Firestore `onSnapshot` listener on `analysis_progress/{docId}`
- Backend updates progress document after each pipeline stage
- Frontend receives real-time updates without polling HTTP endpoints

### Sentiment Models
- **Basic sentiment**: `cardiffnlp/twitter-roberta-base-sentiment` (Positive/Neutral/Negative)
- **Advanced emotions**: GoEmotions model tracking 28 emotions (joy, sadness, anger, fear, surprise, etc.)
- Timeline smoothing applied to reduce noise in sentiment visualizations

### UI Framework
- Tailwind CSS with custom config ([tailwind.config.ts](arcscan/tailwind.config.ts))
- Radix UI components for accessible primitives
- shadcn/ui component library (configured in [components.json](arcscan/components.json))
- Dark mode support via `class` strategy

## File Naming Conventions

- Frontend uses mixed `.tsx` and `.jsx` extensions
- Components that don't use TypeScript features are `.jsx` (e.g., visualization components)
- Page routes and TypeScript-heavy components are `.tsx`
- Backend API routes in Next.js are `.js` files

## Common Patterns

### Protected Routes
Wrap any authenticated page with:
```tsx
import ProtectedRoute from '@/components/ProtectedRoute'

export default function Page() {
  return (
    <ProtectedRoute>
      {/* Your page content */}
    </ProtectedRoute>
  )
}
```

### Firebase Data Access
```tsx
import { db, auth } from '@/lib/firebaseConfig'
import { useAuthState } from 'react-firebase-hooks/auth'
import { collection, doc, getDoc, getDocs } from 'firebase/firestore'
```

### Backend URL Configuration
Next.js API routes use `process.env.BACKEND_URL || 'http://localhost:8000'` to proxy requests to FastAPI.

## Testing & Analysis Scripts

- [`/main.py`](main.py) - Standalone YouTube audio downloader utility
- [`/test_whisper.py`](test_whisper.py) - Test OpenAI Whisper transcription
- [`/firebase_test.py`](firebase_test.py) - Test Firebase connectivity

## Important Notes

- The project is currently in testing phase (per root README)
- Backend expects `firebase_credentials.json` to be present (not in git)
- OpenAI API key required for Whisper and translation features
- FFmpeg must be installed on system for `yt_dlp` audio extraction
- Next.js uses `devIndicators: false` in config to hide dev overlay
